{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"17HE9_KYIc4sp53eMOmIvuptAt_NODoEr","authorship_tag":"ABX9TyPVdaJ+CNilXVTsV1evwlsW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Yzng0bIddbo5","executionInfo":{"status":"ok","timestamp":1681916875337,"user_tz":-420,"elapsed":3689,"user":{"displayName":"Вера Кривоносова","userId":"07769590514536382131"}}},"outputs":[],"source":["from keras.layers import Dropout, Dense,Input,Embedding,Flatten, AveragePooling2D, Conv2D,Reshape, Add\n","from keras.models import Sequential,Model\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np\n","from sklearn import metrics\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.datasets import fetch_20newsgroups\n","import pandas as pd\n","import os\n","\n","MAX_SEQUENCE_LENGTH = 1000\n","MAX_NB_WORDS = 20000\n","EMBEDDING_DIM = 100\n","VALIDATION_SPLIT = 0.2\n","DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/диплом'\n","\n","def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=1000):\n","    np.random.seed(7)\n","    text = np.concatenate((X_train, X_test), axis=0)\n","    text = np.array(text)\n","    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","    tokenizer.fit_on_texts(text)\n","    sequences = tokenizer.texts_to_sequences(text)\n","    word_index = tokenizer.word_index\n","    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","    print('Found %s unique tokens.' % len(word_index))\n","    indices = np.arange(text.shape[0])\n","    # np.random.shuffle(indices)\n","    text = text[indices]\n","    print(text.shape)\n","    X_train = text[0:len(X_train), ]\n","    X_test = text[len(X_train):, ]\n","    embeddings_index = {}\n","    f = open(os.path.join(DATA_DIR, 'glove.6B.100d.txt'), encoding=\"utf8\") ## GloVe file which could be download https://nlp.stanford.edu/projects/glove/\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        try:\n","            coefs = np.asarray(values[1:], dtype='float32')\n","        except:\n","            pass\n","        embeddings_index[word] = coefs\n","    f.close()\n","    print('Total %s word vectors.' % len(embeddings_index))\n","    return (X_train, X_test, word_index,embeddings_index)\n","\n","\n","\n","def Build_Model_CNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=1000, EMBEDDING_DIM=100, dropout=0.5):\n","\n","    \"\"\"\n","        def buildModel_CNN(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n","        word_index in word index ,\n","        embeddings_index is embeddings index, look at data_helper.py\n","        nClasses is number of classes,\n","        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n","        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n","    \"\"\"\n","\n","    model = Sequential()\n","    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n","    for word, i in word_index.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            # words not found in embedding index will be all-zeros.\n","            if len(embedding_matrix[i]) !=len(embedding_vector):\n","                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n","                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n","                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n","                exit(1)\n","\n","            embedding_matrix[i] = embedding_vector\n","\n","    embedding_layer = Embedding(len(word_index) + 1,\n","                                EMBEDDING_DIM,\n","                                weights=[embedding_matrix],\n","                                input_length=MAX_SEQUENCE_LENGTH,\n","                                trainable=True)\n","\n","    # applying a more complex convolutional approach\n","    convs = []\n","    filter_sizes = []\n","    layer = 5\n","    print(\"Filter  \",layer)\n","    for fl in range(0,layer):\n","        filter_sizes.append((fl+2,fl+2))\n","\n","    node = 128\n","    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    emb = Reshape((1000,10, 10), input_shape=(1000,100))(embedded_sequences)\n","\n","    for fsz in filter_sizes:\n","        l_conv = Conv2D(node, padding=\"same\", kernel_size=fsz, activation='relu')(emb)\n","        l_pool = AveragePooling2D(pool_size=(5,1), padding=\"same\")(l_conv)\n","        #l_pool = Dropout(0.25)(l_pool)\n","        convs.append(l_pool)\n","\n","    l_merge = Add()(convs)\n","    l_cov1 = Conv2D(node, (5,5), padding=\"same\", activation='relu')(l_merge)\n","    l_cov1 = AveragePooling2D(pool_size=(5,2), padding=\"same\")(l_cov1)\n","    l_cov2 = Conv2D(node, (5,5), padding=\"same\", activation='relu')(l_cov1)\n","    l_pool2 = AveragePooling2D(pool_size=(5,2), padding=\"same\")(l_cov2)\n","    l_cov2 = Dropout(dropout)(l_pool2)\n","    l_flat = Flatten()(l_cov2)\n","    l_dense = Dense(128, activation='relu')(l_flat)\n","    l_dense = Dropout(dropout)(l_dense)\n","\n","    preds = Dense(nclasses, activation='softmax')(l_dense)\n","    model = Model(sequence_input, preds)\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])\n","\n","\n","\n","    return model"]},{"cell_type":"code","source":["from __future__ import print_function\n","import numpy as np\n","import pandas as pd\n","import pickle\n","from collections import defaultdict\n","import re\n","from bs4 import BeautifulSoup\n","import sys\n","import os\n","\n","os.environ['KERAS_BACKEND'] = 'theano'\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils.np_utils import to_categorical\n","\n","from keras.layers import Embedding\n","from keras.layers import Dense, Input, Flatten\n","from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Add\n","from keras.models import Model\n","\n","MAX_SEQUENCE_LENGTH = 1000\n","MAX_NB_WORDS = 20000\n","EMBEDDING_DIM = 100\n","VALIDATION_SPLIT = 0.2\n","DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/диплом'\n","\n","def clean_str(string):\n","    \"\"\"\n","    Tokenization/string cleaning for dataset\n","    Every dataset is lower cased except\n","    \"\"\"\n","    string = re.sub(r\"\\\\\", \"\", string)    \n","    string = re.sub(r\"\\'\", \"\", string)    \n","    string = re.sub(r\"\\\"\", \"\", string)    \n","    return string.strip().lower()\n","\n","data_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/диплом/export.csv', sep=',')\n","print(data_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quaC4wXYe1Qq","executionInfo":{"status":"ok","timestamp":1681916911139,"user_tz":-420,"elapsed":3543,"user":{"displayName":"Вера Кривоносова","userId":"07769590514536382131"}},"outputId":"9493d8fd-8907-43e2-9780-cd33ba732c5f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["(2716, 7)\n"]}]},{"cell_type":"code","source":["data = pd.DataFrame()\n","data['content'] = data_train['Содержание сообщения']\n","data['label'] = data_train['Оригинал сообщения']\n","data.dropna() \n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"J2pIg8RzfG7h","executionInfo":{"status":"ok","timestamp":1681916917885,"user_tz":-420,"elapsed":1077,"user":{"displayName":"Вера Кривоносова","userId":"07769590514536382131"}},"outputId":"a6ad9d87-21b2-4b30-e27c-cb11078ac2cb"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             content label\n","0  Действительно,кто эту провозгласил примадонной...     0\n","1  Схем много,спасибо,но по всем связать пенсии н...     0\n","2            И найдутся же дураки,кто это чмо купит.     0\n","3  Из бисера делаю эти подснежники,а хотелось бы ...     0\n","4  Да пристрелить уже этого педика ,из ума выжил ...     0"],"text/html":["\n","  <div id=\"df-e22eeeea-3b11-46a8-916f-3f949deefb4a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Действительно,кто эту провозгласил примадонной...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Схем много,спасибо,но по всем связать пенсии н...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>И найдутся же дураки,кто это чмо купит.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Из бисера делаю эти подснежники,а хотелось бы ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Да пристрелить уже этого педика ,из ума выжил ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e22eeeea-3b11-46a8-916f-3f949deefb4a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e22eeeea-3b11-46a8-916f-3f949deefb4a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e22eeeea-3b11-46a8-916f-3f949deefb4a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["texts = []\n","labels = []\n","\n","for idx in range(data.content.shape[0]):\n","    try:\n","        if len(data.label[idx]) == 1:\n","            text = BeautifulSoup(data.content[idx])\n","            # print(data.label[idx])\n","            texts.append(clean_str(text.get_text()))\n","            labels.append(data.label[idx])\n","    except:\n","        print(idx)\n","    \n","\n","tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pk3cKt4yfK0e","executionInfo":{"status":"ok","timestamp":1681916924050,"user_tz":-420,"elapsed":1692,"user":{"displayName":"Вера Кривоносова","userId":"07769590514536382131"}},"outputId":"83549b46-d85a-40e8-ad69-dd45e53df84c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["379\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-7b12a6757fc6>:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  text = BeautifulSoup(data.content[idx])\n"]},{"output_type":"stream","name":"stdout","text":["2626\n","2680\n","2681\n","2682\n","2683\n","2684\n","2685\n","2686\n","2687\n","2688\n","2689\n","2690\n","2691\n","2692\n","2693\n","2694\n","2705\n","Found 13739 unique tokens.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/keras/preprocessing/text.py:246: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["data_pad = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","labels = to_categorical(np.asarray(labels))\n","print(('Shape of data tensor:', data_pad.shape))\n","print(('Shape of label tensor:', labels.shape))\n","\n","indices = np.arange(data_pad.shape[0])\n","np.random.shuffle(indices)\n","data_pad = data_pad[indices]\n","labels = labels[indices]\n","nb_validation_samples = int(VALIDATION_SPLIT * data_pad.shape[0])\n","\n","x_train = data_pad[:-nb_validation_samples]\n","y_train = labels[:-nb_validation_samples]\n","x_val = data_pad[-nb_validation_samples:]\n","y_val = labels[-nb_validation_samples:]\n","\n","print('Number of positive and negative reviews in traing and validation set ')\n","print(y_train.sum(axis=0))\n","print(y_val.sum(axis=0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PGEtJ2sNfLcc","executionInfo":{"status":"ok","timestamp":1681916928862,"user_tz":-420,"elapsed":1105,"user":{"displayName":"Вера Кривоносова","userId":"07769590514536382131"}},"outputId":"87f37c4b-daad-448e-ec72-90b2886b0607"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["('Shape of data tensor:', (2694, 1000))\n","('Shape of label tensor:', (2694, 6))\n","Number of positive and negative reviews in traing and validation set \n","[1943.   27.  165.   10.    3.    8.]\n","[472.   3.  54.   2.   1.   6.]\n"]}]},{"cell_type":"code","source":["X_train_Glove, X_test_Glove, word_index, embeddings_index = loadData_Tokenizer(x_train, x_val)\n","\n","\n","model_CNN = Build_Model_CNN_Text(word_index,embeddings_index, 6)\n","# print(len(X_train_Glove))\n","\n","model_CNN.summary()\n","\n","model_CNN.fit(X_train_Glove, y_train,\n","                              validation_data=(X_test_Glove, y_val),\n","                              epochs=1000,\n","                              batch_size=128,\n","                              verbose=2)\n","\n","predicted = model_CNN.predict(X_test_Glove)\n","\n","predicted = np.argmax(predicted, axis=1)\n","\n","\n","print(metrics.classification_report(y_val, predicted))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"MnvwP9eCflMf","executionInfo":{"status":"error","timestamp":1681916933030,"user_tz":-420,"elapsed":993,"user":{"displayName":"Вера Кривоносова","userId":"07769590514536382131"}},"outputId":"f83f11ae-e616-41c1-c1a0-2392e55db21d"},"execution_count":6,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-265dd976114f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_Glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_Glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadData_Tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_CNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuild_Model_CNN_Text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(len(X_train_Glove))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-a93446027c71>\u001b[0m in \u001b[0;36mloadData_Tokenizer\u001b[0;34m(X_train, X_test, MAX_NB_WORDS, MAX_SEQUENCE_LENGTH)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_NB_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                     seq = text_to_word_sequence(\n\u001b[0m\u001b[1;32m    294\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                         \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \"\"\"\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PV1lAtfRkQpn"},"execution_count":null,"outputs":[]}]}